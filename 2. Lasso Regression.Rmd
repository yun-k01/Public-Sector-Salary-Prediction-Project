---
title: "Lasso Regression"
author: "Yun Kyaw"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
---
## Importing Datasets and Libraries
```{r setup, echo = TRUE, results = "hide", message=FALSE}
library(readr)
library(dplyr)
library(glmnet)
library(caret)
library(fastDummies)
library(tidyr)
library(ModelMetrics)
library(ggplot2)
library(lmtest)
library(MASS) 

hosp_df = read_csv("new_hospital_with_wages.csv")
mun_df = read_csv("new_municipalities_with_wages.csv")
sch_df = read_csv("new_school_with_wages.csv")
```

## Hospital Dataset
```{r}
set.seed(1)

# dropping sector, employer, salary paid, and taxable benefits columns
drop = c("Sector","Employer", "Salary Paid", "Taxable Benefits") 
  
hosp_df = hosp_df[,!(names(hosp_df) %in% drop)] %>%
  drop_na()

# creating dummy variables for job title
df = dummy_cols(hosp_df, remove_selected_columns = TRUE)

# removing outliers from dataset
Q1 = quantile(unlist(df[5]), 0.25)
Q3 = quantile(unlist(df[5]), 0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df = df[unlist(df[5]) >= lower_bound & unlist(df[5]) <= upper_bound, ]

# creating training and testing sets
n = nrow(df)*0.7
random_index = sample(nrow(df), n) # creating a 70-30 partition of training and testing data
hosp_train = df[random_index, ]
hosp_test = df[-random_index, ] 

# selecting predictor variables with interaction terms
x_train_h = data.matrix(hosp_train[-5])
x_train_h_interactions = model.matrix(~ (.)^2 - . - 1, data = as.data.frame(x_train_h))
x_test_h = data.matrix(hosp_test[-5])
x_test_h_interactions = model.matrix(~ (.)^2 - . - 1, data = as.data.frame(x_test_h))

# selecting total compensation as the response variable
y_train_h = as.vector(unlist(hosp_train[5]))
y_test_h = as.vector(unlist(hosp_test[5]))
```


### Creating a Lasso Regression with k-fold cv
```{r}
# Lasso - utilize the training partition to perform validation of results
# selecting lambda using cross validation
set.seed(1)
lasso.cv.out_h = cv.glmnet(x = x_train_h_interactions, y = y_train_h, alpha = 1) 

# using lambda from smallest cross validation error
pred_hosp = predict(lasso.cv.out_h, s = lasso.cv.out_h$lambda.min, newx = x_test_h_interactions)
rmse_hosp = rmse(y_test_h, pred_hosp)

# R2
rss = sum((pred_hosp - y_test_h) ^ 2)  ## residual sum of squares
tss = sum((y_test_h - mean(y_test_h)) ^ 2)  ## total sum of squares
rsq_hosp = 1 - rss/tss
```

### Visualizing the Predictions
```{r}
hosp_data = data.frame('True' = y_test_h, 'Pred' = pred_hosp)

ggplot() +
  geom_point(aes(x = s1, y = True), data = hosp_data, color = "blue") + 
  geom_abline(intercept = 1, slope = 1, color="red") +
  xlab("Predicted") 
```
```{r}
ggplot() +
  geom_density(aes(True, fill = "Actual"), alpha = .2, data = hosp_data) +
  geom_density(aes(s1, fill = "Predicted"), alpha = .2, data = hosp_data) +
  scale_fill_manual(name = "Variable", values = c(Actual = "red", Predicted = "green")) + 
  xlab("Total Compensation")
```


## Municipalities Dataset
```{r}
set.seed(1)

# dropping sector, employer, salary paid, and taxable benefits columns
drop = c("Sector","Employer", "Salary Paid", "Taxable Benefits") 
  
mun_df = mun_df[,!(names(mun_df) %in% drop)] %>%
  drop_na()

# creating dummy variables for job title
df = dummy_cols(mun_df, remove_selected_columns = TRUE)

# removing outliers from dataset
Q1 = quantile(unlist(df[5]), 0.25)
Q3 = quantile(unlist(df[5]), 0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df = df[unlist(df[5]) >= lower_bound & unlist(df[5]) <= upper_bound, ]

# creating training and testing sets
n = nrow(df)*0.7
random_index = sample(nrow(df), n) # creating a 70-30 partition of training and testing data
mun_train = df[random_index, ]
mun_test = df[-random_index, ]

# selecting predictor variables with interaction terms
x_train_m = data.matrix(mun_train[-5])
x_train_m_interactions = model.matrix(~ (.)^2 - . - 1, data = as.data.frame(x_train_m))
x_test_m = data.matrix(mun_test[-5])
x_test_m_interactions = model.matrix(~ (.)^2 - . - 1, data = as.data.frame(x_test_m))

# selecting total compensation as the response variable
y_train_m = as.vector(unlist(mun_train[5]))
y_test_m = as.vector(unlist(mun_test[5]))
```

### Creating a Lasso Regression with k-fold cv
```{r}
# Lasso - utilize the training partition to perform validation of results
# selecting lambda using cross validation
set.seed(1)
lasso.cv.out_m = cv.glmnet(x = x_train_m_interactions, y = y_train_m, alpha = 1) 

# using lambda from smallest cross validation error
pred_mun = predict(lasso.cv.out_m, s = lasso.cv.out_m$lambda.min, newx = x_test_m_interactions)
rmse_mun = rmse(y_test_m, pred_mun)

# R2
rss = sum((pred_mun - y_test_m) ^ 2)  ## residual sum of squares
tss = sum((y_test_m - mean(y_test_m)) ^ 2)  ## total sum of squares
rsq_mun = 1 - rss/tss
```

### Visualizing the Predictions
```{r}
mun_data = data.frame('True' = y_test_m, 'Pred' = pred_mun)

ggplot() +
  geom_point(aes(x = s1, y = True), data = mun_data, color = "blue") + 
  geom_abline(intercept = 1, slope = 1, color="red") +
  xlab("Predicted")
```
```{r}
ggplot() +
  geom_density(aes(True, fill = "Actual"), alpha = .2, data = mun_data) +
  geom_density(aes(s1, fill = "Predicted"), alpha = .2, data = mun_data) +
  scale_fill_manual(name = "Variable", values = c(Actual = "red", Predicted = "green")) + 
  xlab("Total Compensation")
```

## School Dataset
```{r}
set.seed(1)

# dropping sector, employer, salary paid, and taxable benefits columns
drop = c("Sector","Employer", "Salary Paid", "Taxable Benefits") 
  
sch_df = sch_df[,!(names(sch_df) %in% drop)] %>%
  drop_na()

# creating dummy variables for job title
df = dummy_cols(sch_df, remove_selected_columns = TRUE)

# removing outliers from dataset
Q1 = quantile(unlist(df[5]), 0.25)
Q3 = quantile(unlist(df[5]), 0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df = df[unlist(df[5]) >= lower_bound & unlist(df[5]) <= upper_bound, ]

# creating training and testing sets
n = nrow(df)*0.7
random_index = sample(nrow(df), n) # creating a 70-30 partition of training and testing data
sch_train = df[random_index, ]
sch_test = df[-random_index, ]

# selecting predictor variables with interaction terms
x_train_s = data.matrix(sch_train[-5])
x_train_s_interactions = model.matrix(~ (.)^2 - . - 1, data = as.data.frame(x_train_s))
x_test_s = data.matrix(sch_test[-5])
x_test_s_interactions = model.matrix(~ (.)^2 - . - 1, data = as.data.frame(x_test_s))

# selecting total compensation as the response variable
y_train_s = as.vector(unlist(sch_train[5]))
y_test_s = as.vector(unlist(sch_test[5]))
```

### Creating a Lasso Regression with k-fold cv
```{r}
# Lasso - utilize the training partition to perform validation of results
# selecting lambda using cross validation
set.seed(1)
lasso.cv.out_s = cv.glmnet(x = x_train_s_interactions, y = y_train_s, alpha = 1) 

# using lambda from smallest cross validation error
pred_sch = predict(lasso.cv.out_s, s = lasso.cv.out_s$lambda.min, newx = x_test_s_interactions)
rmse_sch = rmse(y_test_s, pred_sch)

# R2
rss = sum((pred_sch - y_test_s) ^ 2)  ## residual sum of squares
tss = sum((y_test_s - mean(y_test_s)) ^ 2)  ## total sum of squares
rsq_sch = 1 - rss/tss
```

### Visualizing the Predictions
```{r}
sch_data = data.frame('True' = y_test_s, 'Pred' = pred_sch)

ggplot() +
  geom_point(aes(x = s1, y = True), data = sch_data, color = "blue") + 
  geom_abline(intercept = 1, slope = 1, color="red") +
  xlab("Predicted")
```

```{r}
ggplot() +
  geom_density(aes(True, fill = "Actual"), alpha = .2, data = sch_data) +
  geom_density(aes(s1, fill = "Predicted"), alpha = .2, data = sch_data) +
  scale_fill_manual(name = "Variable", values = c(Actual = "red", Predicted = "green")) + 
  xlab("Total Compensation")
```

## Comparing the Three Lasso Regression Models
```{r}
# creating a new dataframe
regr_comp = data.frame(
  sector = c("hospitals", "municipalities", "schools"),
  rmse = c(rmse_hosp, rmse_mun, rmse_sch),
  R2 = c(rsq_hosp, rsq_mun, rsq_sch)
)
regr_comp
```
