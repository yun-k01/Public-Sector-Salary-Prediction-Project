---
title: "Lasso Regression"
author: "Yun Kyaw"
date: "`r Sys.Date()`"
output: html_document
---
## Importing Datasets and Libraries
```{r setup, echo = FALSE}
library(readr)
library(dplyr)
library(glmnet)
library(caret)
library(fastDummies)
library(tidyr)
library(ModelMetrics)
library(ggplot2)

hosp_df = read_csv("new_hospital_with_wages.csv")
mun_df = read_csv("new_municipalities_with_wages.csv")
sch_df = read_csv("new_school_with_wages.csv")
```

## Hospital Dataset
```{r}
set.seed(1)

# dropping sector, employer, salary paid, and taxable benefits columns
drop = c("Sector","Employer", "Salary Paid", "Taxable Benefits") 
  
hosp_df = hosp_df[,!(names(hosp_df) %in% drop)] %>%
  drop_na()

# creating dummy variables for job title
df = dummy_cols(hosp_df, remove_selected_columns = TRUE)

n = nrow(df)*0.7
random_index = sample(nrow(df), n) # creating a 70-30 partition of training and testing data
hosp_train = df[random_index, ]
hosp_test = df[-random_index, ] 
```

### Creating a Lasso Regression with k-fold cv
```{r}
# Lasso - utilize the training partition to perform validation of results
x_train_h = data.matrix(hosp_train[-4])
y_train_h = as.vector(unlist(hosp_train[4])) # selecting total compensation as the response variable

x_test_h = data.matrix(hosp_test[-4])
y_test_h = as.vector(unlist(hosp_test[4]))

# selecting lambda using cross validation
lasso.cv.out_h = cv.glmnet(x = x_train_h, y = y_train_h, alpha = 1) 
# plot(lasso.cv.out)

train_hosp = predict(lasso.cv.out_h, s = lasso.cv.out_h$lambda.min, newx = x_train_h)
rmse_hosp_train = rmse(y_train_h, train_hosp)
# rmse_hosp_train = 0.4796117

# using lambda from smallest cross validation error
pred_hosp = predict(lasso.cv.out_h, s = lasso.cv.out_h$lambda.min, newx = x_test_h)
rmse_hosp = rmse(y_test_h, pred_hosp)
# rmse_hosp = 0.4786852
```

## Municipalities Dataset
```{r}
set.seed(1)

# dropping sector, employer, salary paid, and taxable benefits columns
drop = c("Sector","Employer", "Salary Paid", "Taxable Benefits") 
  
mun_df = mun_df[,!(names(mun_df) %in% drop)] %>%
  drop_na()

# creating dummy variables for job title
df = dummy_cols(mun_df, remove_selected_columns = TRUE)

n = nrow(df)*0.7
random_index = sample(nrow(df), n) # creating a 70-30 partition of training and testing data
mun_train = df[random_index, ]
mun_test = df[-random_index, ]  
```

### Creating a Lasso Regression with k-fold cv
```{r}
# Lasso - utilize the training partition to perform validation of results
x_train_m = data.matrix(mun_train[-4])
y_train_m = as.vector(unlist(mun_train[4])) # selecting total compensation as the response variable

x_test_m = data.matrix(mun_test[-4])
y_test_m = as.vector(unlist(mun_test[4]))

# selecting lambda using cross validation
lasso.cv.out_m = cv.glmnet(x = x_train_m, y = y_train_m, alpha = 1) 
# plot(lasso.cv.out)

train_mun = predict(lasso.cv.out_m, s = lasso.cv.out_m$lambda.min, newx = x_train_m)
rmse_mun_train = rmse(y_train_m, train_mun)
# rmse_mun_train = 0.461531

# using lambda from smallest cross validation error
pred_mun = predict(lasso.cv.out_m, s = lasso.cv.out_m$lambda.min, newx = x_test_m)
rmse_mun = rmse(y_test_m, pred_mun)
# rmse_mun = 0.4651846
```

## School Dataset
```{r}
set.seed(1)

# dropping sector, employer, salary paid, and taxable benefits columns
drop = c("Sector","Employer", "Salary Paid", "Taxable Benefits") 
  
sch_df = sch_df[,!(names(sch_df) %in% drop)] %>%
  drop_na()

# creating dummy variables for job title
df = dummy_cols(sch_df, remove_selected_columns = TRUE)

# creating train and test partitions
n = nrow(df)*0.7
random_index = sample(nrow(df), n) # creating a 70-30 partition of training and testing data
sch_train = df[random_index, ]
sch_test = df[-random_index, ]
```

### Creating a Lasso Regression with k-fold cv
```{r}
# Lasso - utilize the training partition to perform validation of results
x_train_s = data.matrix(sch_train[-4])
y_train_s = as.vector(unlist(sch_train[4])) # selecting total compensation as the response variable

x_test_s = data.matrix(sch_test[-4])
y_test_s = as.vector(unlist(sch_test[4]))

# selecting lambda using cross validation
lasso.cv.out_s = cv.glmnet(x = x_train_s, y = y_train_s, alpha = 1) 
# plot(lasso.cv.out)

train_sch = predict(lasso.cv.out_s, s = lasso.cv.out_s$lambda.min, newx = x_train_s)
rmse_sch_train = rmse(y_train_s, train_sch)
# rmse_sch_train = 0.4333067

# using lambda from smallest cross validation error
pred_sch = predict(lasso.cv.out_s, s = lasso.cv.out_s$lambda.min, newx = x_test_s)
rmse_sch = rmse(y_test_s, pred_sch)
# rmse_sch = 0.4341785
```

## Comparing the three regresion models
```{r}
# creating a new dataframe
regr_comp = data.frame(
  sector = c("hospitals", "municipalities", "schools"),
  train_rmse = c(rmse_hosp_train, rmse_mun_train, rmse_sch_train),
  test_rmse = c(rmse_hosp, rmse_mun, rmse_sch)
)
regr_comp
```